{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b404beb9-d633-4cf9-a0a8-ccc47f1fb90c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Evaluating Focal Loss from Huang et al. 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0315f-5a44-4bc4-b8b2-11d4be1c1208",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Huang et al. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80137dbd-5705-4c3f-8f84-2c99525e7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with labels: 10788\n",
      "Number of training documents 6769\n",
      "Number of validation documents 1000\n",
      "Number of testing documents 3019\n"
     ]
    }
   ],
   "source": [
    "# Importing aptemod dataset, getting training, validation, and test splits into same format as Huang et al.\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "def read_labels(labels_path):\n",
    "    \"\"\"Parse labels file into a dict mapping doc_id to list of labels\"\"\"\n",
    "    doc_to_labels = {}\n",
    "    with open(labels_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc_id, label = line.strip().split(' ', 1)\n",
    "            doc_id = doc_id.replace('test/', '')\n",
    "            doc_id = doc_id.replace('training/', '')\n",
    "            doc_to_labels[doc_id] = label.split(' ')\n",
    "    return doc_to_labels\n",
    "\n",
    "def read_document(file_path):\n",
    "    \"\"\"Read a single document, clean its contents, and return them\"\"\"\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        \n",
    "        content = f.read()\n",
    "        content = content.replace('\\n', ' ')\n",
    "        content = ' '.join(content.split())\n",
    "        return content\n",
    "\n",
    "# Read in document ids and associated labels\n",
    "\n",
    "labels_path = os.path.join('reuters-aptemod', 'cats.txt')\n",
    "labels = read_labels(labels_path)\n",
    "\n",
    "print(f\"Number of documents with labels: {len(labels)}\")\n",
    "\n",
    "# Read in document texts\n",
    "\n",
    "training_path = os.path.join('reuters-aptemod', 'training')\n",
    "data_train_all = []\n",
    "for file in os.listdir(training_path):\n",
    "    if file in labels:\n",
    "        file_dict = {\n",
    "            'text': read_document(os.path.join(training_path, file)),\n",
    "            'labels': labels[file]\n",
    "        }\n",
    "        data_train_all.append(file_dict)\n",
    "\n",
    "test_path = os.path.join('reuters-aptemod', 'test')\n",
    "data_test = []\n",
    "for file in os.listdir(test_path):\n",
    "    file_dict = {}\n",
    "    if file in labels:\n",
    "        file_dict = {\n",
    "            'text': read_document(os.path.join(test_path, file)),\n",
    "            'labels': labels[file]\n",
    "        }\n",
    "        data_test.append(file_dict)\n",
    "\n",
    "# Split validation data from training data. \n",
    "\n",
    "data_train, data_validation = train_test_split(data_train_all, random_state = 100, test_size = 1000) # Using a different random seed relative to Huang et al. because their seed of 123 was splitting my 'data_train_all' variable such that the training set was missing a single label, 'groundnut-oil'. This discrepancy occurs despite the similar seed because our 'data_train_all' variable has its documents in a different order than what Huang et al. originally had. I could not determine the exact order in which Huang et al. had their training documents in prior to splitting off validation data, but this should not be a big issue so long as our training set still has all 90 labels. The results of the various loss functions should not vary greatly from Huang et al.'s original results since we're just working with a slightly different variation of their original split.\n",
    "\n",
    "print(f\"Number of training documents {len(data_train)}\")\n",
    "\n",
    "print(f\"Number of validation documents {len(data_validation)}\")\n",
    "\n",
    "print(f\"Number of testing documents {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fde05c7-71c8-4ea1-9042-651fdf265dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in cats.txt: 90\n",
      "Labels are: ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "# Making sure number of unique labels in the entire dataset is 90\n",
    "\n",
    "unique_labels = set()\n",
    "for label_list in labels.values():\n",
    "    unique_labels.update(label_list)\n",
    "print(f\"Number of unique labels in cats.txt: {len(unique_labels)}\")\n",
    "print(f\"Labels are: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296c6a93-c336-43c7-bf60-96e71ad0b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 90\n"
     ]
    }
   ],
   "source": [
    "term2count = Counter([x for docu in data_train for x in docu['labels']])\n",
    "FREQ_CUTOFF = 0 \n",
    "term_freq = sorted([term for term, count in term2count.items() if count>=FREQ_CUTOFF])\n",
    "labels_ref = sorted([z for z in set([y for x in data_train for y in x['labels']]) if z in term_freq]) \n",
    "print(len(term2count), len(labels_ref))\n",
    "class_freq = [term2count[x] for x in labels_ref]\n",
    "train_num = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a5e00-4d18-4e36-b4a9-a61ae3458c7d",
   "metadata": {},
   "source": [
    "### Huang et al. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f900e6-f22b-42f9-9aee-e6d2f17cc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_loss import ResampleLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf093734-b11d-4e7c-bff6-d39aa152bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from util_loss import ResampleLoss\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c05ba00-029d-47ef-aff1-97db1601684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initializing model and tokenizer\n",
    "num_labels = len(labels_ref)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc969b38-7191-44a9-bb2a-ac7d1b3fdbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=90, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up device and move model to it\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2adc15a-3258-456c-9ea2-2d6f4431c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manlai\\anaconda3\\envs\\TextClassification\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining optimizer\n",
    "# Our own original experiments did not use grouped parameters to define which parameters should and shouldn't have weight decay applied. This is clearly a step forward relative to our model in that it allows more flexibility in terms of fine-tuning.\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4) # the learning rate applied is also different relative to our own experiments from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3d3b58-0c23-4a18-a1f0-445248fe6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Focal-Loss Function according to Huang et al. methodology\n",
    "loss_func = ResampleLoss(\n",
    "    reweight_func=None,\n",
    "    loss_weight=1.0,\n",
    "    focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "    logit_reg=dict(),\n",
    "    class_freq=class_freq,\n",
    "    train_num=train_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae8990f-6e30-4736-a0df-a20e52c17ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(docu):\n",
    "    labels = [1 if x in docu['labels'] else 0 for x in labels_ref]\n",
    "    encodings = tokenizer(\n",
    "        docu['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'].flatten(),\n",
    "        'attention_mask': encodings['attention_mask'].flatten(),\n",
    "        'labels': torch.tensor(labels, dtype=torch.float)\n",
    "    }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return preprocess_function(self.documents[index])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(data_train)\n",
    "val_dataset = CustomDataset(data_validation)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88803ee3-d078-4c03-bfd5-1823f05a185b",
   "metadata": {},
   "source": [
    "### Huang et al. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8c02ac-12cb-45b4-972b-838b2d4f6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                    | 0/40 [02:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m training_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     26\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;66;03m# Moving tensors in batch to GPU\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     b_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TextClassification\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TextClassification\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TextClassification\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TextClassification\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preprocess_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments[index])\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[1;34m(docu)\u001b[0m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m docu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m labels_ref]\n\u001b[0;32m      3\u001b[0m encodings \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      4\u001b[0m     docu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     14\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create training loop as per Huang et al.\n",
    "source_dir = './'\n",
    "\n",
    "prefix = 'reuters'\n",
    "loss_func_name = 'FL'\n",
    "suffix = 'rand100'\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "epochs = 40 # Epoch count utilized by Huang et al.\n",
    "best_f1_for_epoch = 0 # Tracking best f1 score\n",
    "epochs_without_improvement = 0 # Implementing early stop if loss does not improve\n",
    "\n",
    "# Create directories if they don't already exist\n",
    "model_dir = os.path.join(source_dir, 'models')\n",
    "log_dir = os.path.join(source_dir, 'logs')\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)  # Creates models directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)    # Creates logs directory if it doesn't exist\n",
    "\n",
    "for epoch in trange(epochs, desc='Epoch'): # Using trange from the tqdm library for the progress bar. \n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} # Moving tensors in batch to GPU\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_labels = batch['labels']\n",
    "        optimizer.zero_grad() # Clearing gradients from prior batch, prevent accumulation across batches\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # passing input into BERT model to retrieve logits\n",
    "        logits = outputs[0]\n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) # calculating loss via the loss function we specified from the util_loss module's ResampleLoss class, in this case just regular BCE. Additionally, we're reshaping the logits to match the labels, converting labels to match the same data type as the logits, and also reshaping them.\n",
    "        loss.backward() # Computing gradients\n",
    "        optimizer.step() # Updating weights\n",
    "        training_loss += loss.item() # Summing training loss\n",
    "        training_steps += 1 # Counting training steps\n",
    "        \n",
    "    print(\"Train loss: {}\".format(training_loss/training_steps))\n",
    "    \n",
    "    # Validation section\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "    true_labels,pred_labels = [],[]\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_labels = batch['labels']\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred) # Applying sigmoid to logits to acquire probabilities\n",
    "            loss = loss_func(b_logit_pred.view(-1,num_labels),b_labels.type_as(b_logit_pred).view(-1,num_labels))\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "            \n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "            \n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    print(\"Validation loss: {}\".format(val_loss/val_steps))\n",
    "    \n",
    "    # Flatten outputs into 1d lists.\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.5\n",
    "    true_bools = [tl==1 for tl in true_labels] # turning actual labels into booleans\n",
    "    pred_bools = [pl>threshold for pl in pred_labels] # predicting labels based on threshold\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "    val_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "    val_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Precision Validation Accuracy: ', val_precision_accuracy)\n",
    "    print('Recall Validation Accuracy: ', val_recall_accuracy)\n",
    "    \n",
    "    # Calculate AUC as well, will need to look into this some more as I'm unsure what this is exactly\n",
    "    val_auc_score = roc_auc_score(true_bools, pred_labels, average='micro')\n",
    "    print('AUC Validation: ', val_auc_score)\n",
    "    \n",
    "    # Searching for best Threshold for f1. Essentially, what's going on here is that we're creating a range of thresholds from 0.4 to 0.6 with steps of 0.01 in between. Then, we're looping over this range and testing for which threshold yields the highest f1 score, printing that which gives the best results. \n",
    "    best_med_th = 0.5\n",
    "    micro_thresholds = (np.array(range(-10,11))/100)+best_med_th\n",
    "    f1_results, prec_results, recall_results = [], [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "        test_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        prec_results.append(test_precision_accuracy)\n",
    "        recall_results.append(test_recall_accuracy)\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "    \n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    \n",
    "    # Save the model if this epoch gives the best f1 score in validation set\n",
    "    if f1_results[best_f1_idx] > (best_f1_for_epoch * 0.995):\n",
    "        best_f1_for_epoch = f1_results[best_f1_idx]\n",
    "        epochs_without_improvement = 0\n",
    "        for fname in os.listdir(model_dir):\n",
    "            if fname.startswith('_'.join([prefix,model_name,loss_func_name,suffix])):\n",
    "                os.remove(os.path.join(model_dir, fname))\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'para'))\n",
    "    else:\n",
    "        epochs_without_improvement += 1    \n",
    "        \n",
    "    # Log all results in validation set with different thresholds\n",
    "    with open(os.path.join(log_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'.json'),'w') as f:\n",
    "        d = {}\n",
    "        d[\"f1_accuracy_default\"] =  val_f1_accuracy\n",
    "        d[\"pr_accuracy_default\"] =  val_precision_accuracy\n",
    "        d[\"rec_accuracy_default\"] =  val_recall_accuracy\n",
    "        d[\"auc_score_default\"] =  val_auc_score\n",
    "        d[\"thresholds\"] =  list(micro_thresholds)\n",
    "        d[\"threshold_f1s\"] =  f1_results\n",
    "        d[\"threshold_precs\"] =  prec_results\n",
    "        d[\"threshold_recalls\"] =  recall_results\n",
    "        json.dump(d, f)\n",
    "    \n",
    "    # If 5 epochs pass without improvement consider the model as saturated and exit\n",
    "    if epochs_without_improvement > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc873bc8-4b04-4b94-82c9-2ecbc175b940",
   "metadata": {},
   "source": [
    "### Huang et al. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceea7c2-7a7b-47cc-840f-957267db88ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
