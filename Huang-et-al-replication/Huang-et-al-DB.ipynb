{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc5e693-b34a-4b20-9f04-a024603e5fbb",
   "metadata": {},
   "source": [
    "# Training and Evaluating Distribution Balanced Loss from Huang et al. 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0315f-5a44-4bc4-b8b2-11d4be1c1208",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Huang et al. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80137dbd-5705-4c3f-8f84-2c99525e7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing aptemod dataset, getting training, validation, and test splits into same format as Huang et al.\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "def read_labels(labels_path):\n",
    "    \"\"\"Parse labels file into a dict mapping doc_id to list of labels\"\"\"\n",
    "    doc_to_labels = {}\n",
    "    with open(labels_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc_id, label = line.strip().split(' ', 1)\n",
    "            doc_id = doc_id.replace('test/', '')\n",
    "            doc_id = doc_id.replace('training/', '')\n",
    "            doc_to_labels[doc_id] = label.split(' ')\n",
    "    return doc_to_labels\n",
    "\n",
    "def read_document(file_path):\n",
    "    \"\"\"Read a single document, clean its contents, and return them\"\"\"\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        \n",
    "        content = f.read()\n",
    "        content = content.replace('\\n', ' ')\n",
    "        content = ' '.join(content.split())\n",
    "        return content\n",
    "\n",
    "# Read in document ids and associated labels\n",
    "\n",
    "labels_path = os.path.join('reuters-aptemod', 'cats.txt')\n",
    "labels = read_labels(labels_path)\n",
    "\n",
    "print(f\"Number of documents with labels: {len(labels)}\")\n",
    "\n",
    "# Read in document texts\n",
    "\n",
    "training_path = os.path.join('reuters-aptemod', 'training')\n",
    "data_train_all = []\n",
    "for file in os.listdir(training_path):\n",
    "    if file in labels:\n",
    "        file_dict = {\n",
    "            'text': read_document(os.path.join(training_path, file)),\n",
    "            'labels': labels[file]\n",
    "        }\n",
    "        data_train_all.append(file_dict)\n",
    "\n",
    "test_path = os.path.join('reuters-aptemod', 'test')\n",
    "data_test = []\n",
    "for file in os.listdir(test_path):\n",
    "    file_dict = {}\n",
    "    if file in labels:\n",
    "        file_dict = {\n",
    "            'text': read_document(os.path.join(test_path, file)),\n",
    "            'labels': labels[file]\n",
    "        }\n",
    "        data_test.append(file_dict)\n",
    "\n",
    "# Split validation data from training data. \n",
    "\n",
    "data_train, data_validation = train_test_split(data_train_all, random_state = 100, test_size = 1000) # Using a different random seed relative to Huang et al. because their seed of 123 was splitting my 'data_train_all' variable such that the training set was missing a single label, 'groundnut-oil'. This discrepancy occurs despite the similar seed because our 'data_train_all' variable has its documents in a different order than what Huang et al. originally had. I could not determine the exact order in which Huang et al. had their training documents in prior to splitting off validation data, but this should not be a big issue so long as our training set still has all 90 labels. The results of the various loss functions should not vary greatly from Huang et al.'s original results since we're just working with a slightly different variation of their original split.\n",
    "\n",
    "print(f\"Number of training documents {len(data_train)}\")\n",
    "\n",
    "print(f\"Number of validation documents {len(data_validation)}\")\n",
    "\n",
    "print(f\"Number of testing documents {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde05c7-71c8-4ea1-9042-651fdf265dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure number of unique labels in the entire dataset is 90\n",
    "\n",
    "unique_labels = set()\n",
    "for label_list in labels.values():\n",
    "    unique_labels.update(label_list)\n",
    "print(f\"Number of unique labels in cats.txt: {len(unique_labels)}\")\n",
    "print(f\"Labels are: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c6a93-c336-43c7-bf60-96e71ad0b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "term2count = Counter([x for docu in data_train for x in docu['labels']])\n",
    "FREQ_CUTOFF = 0 \n",
    "term_freq = sorted([term for term, count in term2count.items() if count>=FREQ_CUTOFF])\n",
    "labels_ref = sorted([z for z in set([y for x in data_train for y in x['labels']]) if z in term_freq]) \n",
    "print(len(term2count), len(labels_ref))\n",
    "class_freq = [term2count[x] for x in labels_ref]\n",
    "train_num = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7dfa8-3bc1-439b-8ddd-927d5ec231f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Huang et al. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f900e6-f22b-42f9-9aee-e6d2f17cc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_loss import ResampleLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf093734-b11d-4e7c-bff6-d39aa152bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from util_loss import ResampleLoss\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05ba00-029d-47ef-aff1-97db1601684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model and tokenizer\n",
    "num_labels = len(labels_ref)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc969b38-7191-44a9-bb2a-ac7d1b3fdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device and move model to it\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adc15a-3258-456c-9ea2-2d6f4431c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer\n",
    "# Our own original experiments did not use grouped parameters to define which parameters should and shouldn't have weight decay applied. This is clearly a step forward relative to our model in that it allows more flexibility in terms of fine-tuning.\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4) # the learning rate applied is also different relative to our own experiments from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d3b58-0c23-4a18-a1f0-445248fe6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Distribution Balanced Loss Function according to Huang et al. methodology\n",
    "loss_func = ResampleLoss(\n",
    "    reweight_func='rebalance',\n",
    "    loss_weight=1.0,\n",
    "    focal=dict(focal=True, alpha=0.5, gamma=2),\n",
    "    logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n",
    "    map_param=dict(alpha=0.1, beta=10.0, gamma=0.9),\n",
    "    class_freq=class_freq,\n",
    "    train_num=train_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8990f-6e30-4736-a0df-a20e52c17ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(docu):\n",
    "    labels = [1 if x in docu['labels'] else 0 for x in labels_ref]\n",
    "    encodings = tokenizer(\n",
    "        docu['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'].flatten(),\n",
    "        'attention_mask': encodings['attention_mask'].flatten(),\n",
    "        'labels': torch.tensor(labels, dtype=torch.float)\n",
    "    }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return preprocess_function(self.documents[index])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(data_train)\n",
    "val_dataset = CustomDataset(data_validation)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a044c-1479-4190-a09f-e452ae3a2318",
   "metadata": {},
   "source": [
    "### Huang et al. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c02ac-12cb-45b4-972b-838b2d4f6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training loop as per Huang et al.\n",
    "source_dir = './'\n",
    "\n",
    "prefix = 'reuters'\n",
    "loss_func_name = 'DBL'\n",
    "suffix = 'rand100'\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "epochs = 40 # Epoch count utilized by Huang et al.\n",
    "best_f1_for_epoch = 0 # Tracking best f1 score\n",
    "epochs_without_improvement = 0 # Implementing early stop if loss does not improve\n",
    "\n",
    "# Create directories if they don't already exist\n",
    "model_dir = os.path.join(source_dir, 'models')\n",
    "log_dir = os.path.join(source_dir, 'logs')\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)  # Creates models directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)    # Creates logs directory if it doesn't exist\n",
    "\n",
    "for epoch in trange(epochs, desc='Epoch'): # Using trange from the tqdm library for the progress bar. \n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} # Moving tensors in batch to GPU\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_labels = batch['labels']\n",
    "        optimizer.zero_grad() # Clearing gradients from prior batch, prevent accumulation across batches\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # passing input into BERT model to retrieve logits\n",
    "        logits = outputs[0]\n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) # calculating loss via the loss function we specified from the util_loss module's ResampleLoss class, in this case just regular BCE. Additionally, we're reshaping the logits to match the labels, converting labels to match the same data type as the logits, and also reshaping them.\n",
    "        loss.backward() # Computing gradients\n",
    "        optimizer.step() # Updating weights\n",
    "        training_loss += loss.item() # Summing training loss\n",
    "        training_steps += 1 # Counting training steps\n",
    "        \n",
    "    print(\"Train loss: {}\".format(training_loss/training_steps))\n",
    "    \n",
    "    # Validation section\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "    true_labels,pred_labels = [],[]\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        b_input_ids = batch['input_ids']\n",
    "        b_input_mask = batch['attention_mask']\n",
    "        b_labels = batch['labels']\n",
    "        with torch.no_grad():\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred) # Applying sigmoid to logits to acquire probabilities\n",
    "            loss = loss_func(b_logit_pred.view(-1,num_labels),b_labels.type_as(b_logit_pred).view(-1,num_labels))\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "            \n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "            \n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    print(\"Validation loss: {}\".format(val_loss/val_steps))\n",
    "    \n",
    "    # Flatten outputs into 1d lists.\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.5\n",
    "    true_bools = [tl==1 for tl in true_labels] # turning actual labels into booleans\n",
    "    pred_bools = [pl>threshold for pl in pred_labels] # predicting labels based on threshold\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "    val_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "    val_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Precision Validation Accuracy: ', val_precision_accuracy)\n",
    "    print('Recall Validation Accuracy: ', val_recall_accuracy)\n",
    "    \n",
    "    # Calculate AUC as well, will need to look into this some more as I'm unsure what this is exactly\n",
    "    val_auc_score = roc_auc_score(true_bools, pred_labels, average='micro')\n",
    "    print('AUC Validation: ', val_auc_score)\n",
    "    \n",
    "    # Searching for best Threshold for f1. Essentially, what's going on here is that we're creating a range of thresholds from 0.4 to 0.6 with steps of 0.01 in between. Then, we're looping over this range and testing for which threshold yields the highest f1 score, printing that which gives the best results. \n",
    "    best_med_th = 0.5\n",
    "    micro_thresholds = (np.array(range(-10,11))/100)+best_med_th\n",
    "    f1_results, prec_results, recall_results = [], [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_precision_accuracy = precision_score(true_bools, pred_bools,average='micro')\n",
    "        test_recall_accuracy = recall_score(true_bools, pred_bools,average='micro')\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        prec_results.append(test_precision_accuracy)\n",
    "        recall_results.append(test_recall_accuracy)\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "    \n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Accuracy: ', f1_results[best_f1_idx])\n",
    "    \n",
    "    # Save the model if this epoch gives the best f1 score in validation set\n",
    "    if f1_results[best_f1_idx] > (best_f1_for_epoch * 0.995):\n",
    "        best_f1_for_epoch = f1_results[best_f1_idx]\n",
    "        epochs_without_improvement = 0\n",
    "        for fname in os.listdir(model_dir):\n",
    "            if fname.startswith('_'.join([prefix,model_name,loss_func_name,suffix])):\n",
    "                os.remove(os.path.join(model_dir, fname))\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'para'))\n",
    "    else:\n",
    "        epochs_without_improvement += 1    \n",
    "        \n",
    "    # Log all results in validation set with different thresholds\n",
    "    with open(os.path.join(log_dir, '_'.join([prefix,model_name,loss_func_name,suffix,'epoch'])+str(epoch+1)+'.json'),'w') as f:\n",
    "        d = {}\n",
    "        d[\"f1_accuracy_default\"] =  val_f1_accuracy\n",
    "        d[\"pr_accuracy_default\"] =  val_precision_accuracy\n",
    "        d[\"rec_accuracy_default\"] =  val_recall_accuracy\n",
    "        d[\"auc_score_default\"] =  val_auc_score\n",
    "        d[\"thresholds\"] =  list(micro_thresholds)\n",
    "        d[\"threshold_f1s\"] =  f1_results\n",
    "        d[\"threshold_precs\"] =  prec_results\n",
    "        d[\"threshold_recalls\"] =  recall_results\n",
    "        json.dump(d, f)\n",
    "    \n",
    "    # If 5 epochs pass without improvement consider the model as saturated and exit\n",
    "    if epochs_without_improvement > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128811b-7a3b-401e-bbf9-a6bb3bd9b42f",
   "metadata": {},
   "source": [
    "### Huang et al. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9580a-6ade-4d6a-be46-f525d48acb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
